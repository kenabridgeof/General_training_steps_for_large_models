import torch
import torch.nn as nn
import math

# DemoEmbedding ç»§æ‰¿è‡ª nn.Moduleï¼Œæ˜¯ä¸€ä¸ªâ€œTokenâ†’å‘é‡â€æ˜ å°„æ¨¡å—ï¼Œé€šå¸¸ç”¨äº NLP æ¨¡å‹çš„è¯åµŒå…¥ï¼ˆword embeddingï¼‰å±‚ã€‚

"""
æ„é€ å‡½æ•° __init__

1. vocab
    è¯è¡¨å¤§å°ï¼ˆtoken æ•°é‡ï¼‰ï¼Œä¾‹å¦‚å¦‚æœæœ‰ 50,000 ä¸ªä¸åŒå•è¯/å­è¯ï¼Œå°±ä¼ å…¥ vocab=50000ã€‚
2. embed_dim
    è¦æŠŠæ¯ä¸ª token æ˜ å°„åˆ°çš„å‘é‡ç»´åº¦ï¼ˆæ¯”å¦‚ 512ã€768 ç­‰ï¼‰ã€‚
3. self.embed = nn.Embedding(vocab, embed_dim)
    PyTorch å†…ç½®çš„æŸ¥è¡¨å¼åµŒå…¥å±‚ï¼š
        å®ƒå†…éƒ¨ç»´æŠ¤ä¸€ä¸ªå½¢çŠ¶ä¸º [vocab, embed_dim] çš„å¯è®­å‚æ•°çŸ©é˜µã€‚
        å½“è°ƒç”¨ self.embed(x) æ—¶ï¼Œx æ˜¯ä¸€ä¸ªæ•´å‹å¼ é‡ï¼ˆtoken ç´¢å¼•ï¼‰ï¼Œå±‚å°±ä¼šæŠŠæ¯ä¸ªç´¢å¼•æ˜ å°„åˆ°å¯¹åº”è¡Œï¼ˆå‘é‡ï¼‰ï¼Œè¾“å‡ºä¸€ä¸ªæµ®ç‚¹å‘é‡ã€‚
"""
"""
å‰å‘å‡½æ•° forward
ç›®çš„: 
    å°†â€œåŸå§‹çš„â€åµŒå…¥å‘é‡çš„æ•°å€¼å°ºåº¦æ”¾å¤§åˆ°å’Œä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰ç›¸å½“çš„é‡çº§ ã€‚
ä¸ºä»€ä¹ˆè¦è¿™æ ·:
    å¦‚æœç›´æ¥æŠŠæœªç»æ”¾ç¼©çš„ embeddingï¼ˆå®ƒä»¬é€šå¸¸æ˜¯å‡å€¼ä¸º 0ã€æ–¹å·®å°äº 1 çš„éšæœºå‘é‡ï¼‰åŠ åˆ°ä½ç½®ç¼–ç ä¸Šï¼Œä½ç½®ç¼–ç åè€Œä¼šä¸»å¯¼æ¨¡å‹å­¦ä¹ ã€‚
ä¹˜ä»¥ âˆšğ‘‘â‚– åï¼Œè¿™ä¸¤ä¸ªåŠ æ•°æ–¹å·®å·®ä¸å¤šï¼Œæ¨¡å‹èƒ½æ›´å¥½åœ°åŒæ—¶åˆ©ç”¨â€œè¯­ä¹‰ä¿¡æ¯â€ï¼ˆtoken embeddingï¼‰å’Œâ€œä½ç½®ä¿¡æ¯â€ï¼ˆpositional encodingï¼‰ã€‚
"""

class DemoEmbedding(nn.Module):
    def __init__(self, vocab, embed_dim):
        super().__init__()
        # vocab
        self.vocab = vocab
        self.d_model = embed_dim

        self.embed = nn.Embedding(self.vocab, self.d_model)
        

    def forward(self, x):
        return self.embed(x) * math.sqrt(self.d_model)
