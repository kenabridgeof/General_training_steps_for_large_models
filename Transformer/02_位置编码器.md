### 

```python
class PositionEncoding(nn.Module):
    def __init__(self, d_model, dropout_p, max_len=60):
        super().__init__()
        # 0.初始化dropout_p,并且nn.Dropout
        self.dropout_p = dropout_p
        self.dropout = nn.Dropout(p=dropout_p)
        # 1.全零初始化PE [60, 512]
        pe = torch.zeros(max_len, d_model)
        # 2.初始化position [60, 1]
        pos = torch.arange(0, max_len).unsqueeze(1)
        # 3.初始化pos除以的内容
        div_term = torch.exp(torch.arange(0, d_model, 2) * math.log(10000)/d_model)
        # 4.相除,也就是相乘 [60, 256]
        my_matmulres = pos * div_term
        # 5.拼接sin,cos,记得2i和2i + 1要切片表达式
        pe[:, 0::2] = torch.sin(my_matmulres)
        pe[:, 1::2] = torch.cos(my_matmulres)
        # 6.升到3维,以便计算
        pe.unsqueeze(0)
        # 7.存入buffer, 以便在 .to(device)、保存/加载时自动跟踪，但不参与梯度更新。
        self.register_buffer('pe', pe)

    def forward(self, x):
        # 1.因为输入需要经过embedding层的语义向量x和位置编码加起来
        x = x + self.pe[:, x.size()[1]]
        # 2.返回融合的随机失活x
        return self.dropout(x)
```



### 
