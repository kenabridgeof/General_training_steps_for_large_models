# 手机价格分类深度学习流程

### 可调超参数

* **batch\_size**：DataLoader 每批次样本数，影响显存和迭代稳定性。常用 16、32、64。
* **epochs**：训练轮数，总梯度更新次数。常用 50、100、200。
* **learning\_rate**：优化器初始学习率，控制参数更新步长。常用 1e-3、5e-4。
* **optimizer**：优化算法类型，如 Adam(β=(0.9,0.99))、SGD+Momentum(0.9)。
* **random\_state**：用于数据集划分和随机操作的种子值，保证实验可复现。常用 24、42。
* **hidden\_dims**：MLP 隐藏层单元数列表，决定模型容量，常用 \[128,64]。
* **dropout\_p**：Dropout 丢弃概率，用于正则化，阻止过拟合，常用 0.1–0.5。
* **weight\_decay**：L2 正则化系数，防止模型过拟合，常用 1e-5–1e-3。

---

## 1. 数据加载与预处理

* **原始数据读取**：通过 Pandas 将 CSV 文件加载为 DataFrame，提取特征列和标签列。

* **数据类型转换**：将 Pandas 数据转为 NumPy，再使用张量封装，确保特征张量 dtype=float32，标签张量 dtype=int64。

* **数据集划分**：利用 sklearn 的 train\_test\_split 或 PyTorch SubsetRandomSampler，将数据分为训练集与验证集，根据 random\_state 固定划分随机性。

* **特征预处理**：可选对数值特征进行标准化/归一化，分类特征进行编码，图像数据可做增强（裁剪、翻转、归一化）。

* **封装 Dataset**：使用 TensorDataset 或自定义 Dataset 类，将特征张量和标签张量打包，支持 **len** 和 **getitem** 接口。

* **构建 DataLoader**：为 Dataset 创建 DataLoader，配置 batch\_size、shuffle、num\_workers、pin\_memory 等参数，实现小批量并行加载。

---

## 2. 模型定义

本项目支持多种网络结构，包括 多层感知机（MLP）、卷积神经网络（CNN）和循环神经网络（RNN），可根据任务需求灵活选择。

### 2.1 多层感知机（MLP）

1. **组件声明**

   * 输入映射层：将特征维度映射到首个隐藏单元；
   * 隐藏模块（可重复 N 层）：每个模块依次包含“线性变换→归一化层→激活层→Dropout”；
   * 输出映射层：将最后隐藏表示映射到类别数。
2. **前向计算**

   * 输入送入输入映射层；
   * 隐藏层模块逐层执行线性变换、BatchNorm/LayerNorm、ReLU/GELU、Dropout；
   * 最终送入输出映射层，生成分类 logits。

### 2.2 卷积神经网络（CNN）

1. **卷积块设计**

   * 每个卷积块顺序包含“二维卷积→激活→池化（可选归一化）”；
   * 卷积核通道数逐块增加，用于捕捉从低级到高级的空间特征。
2. **特征展平**

   * 卷积+池化后得到的多通道特征图展平成一维向量；
   * 展平长度为 通道数×高×宽，用于后续全连接层。
3. **分类头**

   * 若干全连接层组合（线性→归一化→激活→Dropout），映射至最终类别；
   * 输出 logits 用于计算交叉熵损失。

### 2.3 循环神经网络（RNN）

1. **序列输入**

   * 将输入特征序列按时间步张量化，形状 `[batch_size, seq_len, feature_dim]`；
2. **循环单元**

   * 使用 `LSTM` 或 `GRU` 等循环模块，逐步处理时序信息；
   * 能捕获长短期依赖关系，内部包含门控机制控制信息流。
3. **序列聚合**

   * 可取最后时刻的隐藏状态，或对所有时刻输出做平均/最大池化，得到固定维度表示；
4. **分类映射**

   * 将聚合后的表示输入全连接层 + 激活 + Dropout → 输出分类 logits。

### 2.4 参数初始化

* 对所有线性层和卷积层权重使用 Xavier/He 初始化；
* 对偏置统一归零，确保初始无偏。

---

## 3. 训练流程

* **切换训练模式**：启用 model.train()，确保正则化层（Dropout、BatchNorm）按训练模式工作。

* **损失函数配置**：使用适合分类的交叉熵损失 (CrossEntropyLoss)。

* **优化器与调度**：构造 Adam 或 SGD+Momentum 优化器，设置 learning\_rate、weight\_decay；配合学习率调度器（如 CosineAnnealingLR、StepLR、Warmup）动态调整学习率。

* **梯度裁剪**：在反向传播前对梯度范数进行裁剪（clip\_grad\_norm\_），避免梯度爆炸。

* **迭代更新**：对每个 epoch：

  1. 遍历训练 DataLoader 批次；
  2. 前向计算输出；
  3. 计算损失并反向传播；
  4. 执行优化器 step 更新参数；
  5. 清零梯度，准备下一步。

* **日志记录**：定期记录训练损失、学习率、梯度统计等指标，可视化于 TensorBoard 或 WandB。

* **模型检查点**：在每个 epoch 结束后或验证指标提升时保存最佳权重，便于后续恢复或推理。

---

## 4. 验证与评估

* **切换评估模式**：启用 model.eval() 并在 torch.no\_grad() 环境中关闭梯度计算。

* **性能指标**：计算预测准确率 (Accuracy)、精确率/召回率/F1 (Precision/Recall/F1)、AUC、混淆矩阵 (ConfusionMatrix) 等。

* **分类报告**：借助 sklearn.metrics 输出各类别精度、召回、F1 报告，并生成混淆矩阵可视化。

* **可视化分析**：绘制 ROC 曲线、PR 曲线，展示典型错分样本，辅助模型调优。

---

## 5. 导出与部署

* **模型序列化**：保存 state\_dict 或导出 TorchScript 模型，以支持 C++/Java 推理。

* **格式转换**：根据需求将模型导出为 ONNX、TensorRT 引擎或 OpenVINO 格式，优化推理性能。

* **服务封装**：基于 FastAPI、Flask 或 TorchServe 制作 RESTful 接口，提供批量/单样推理。

* **容器化部署**：使用 Docker 构建镜像，集成依赖与环境变量，实现跨平台部署。

* **线上监控**：部署监控系统收集预测延迟、吞吐量和准确率，结合日志触发模型再训练或微调流程。
