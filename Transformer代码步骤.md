# Transformer æ ¸å¿ƒæ¨¡å—ç»“æ„ç´¢å¼•
```
# -*-coding:utf-8-*-
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
import copy
import matplotlib.pyplot as plt
```

# ä¸€.Input
---

## 1. Embedding(è¯åµŒå…¥å±‚)

- **ç±»å**ï¼š`Embedding`
- **æ„é€ å‡½æ•°**ï¼š`__init__(vocab_size, d_model)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x)`
- **æ³¨æ„**ï¼š`forward` è¾“å‡ºéœ€ä¹˜ä»¥ $\sqrt{d_\text{model}}$

---

## 2. PositionEncoding(ä½ç½®ç¼–ç )
ä½œç”¨ï¼šä¸º Transformer æä¾›ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼Œè®©æ¨¡å‹çŸ¥é“åºåˆ—ä¸­æ¯ä¸ª token çš„ä½ç½®ä¿¡æ¯ã€‚
- **ç±»å**ï¼š`PositionEncoding`
- **æ„é€ å‡½æ•°**ï¼š`__init__(d_model, dropout_p, max_len=...)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x)`
- **å…¬å¼**:<img src="https://github.com/user-attachments/assets/f6ba8457-bbf8-4314-8e12-91e2b6d58aab" alt="ä½ç½®ç¼–ç è®¡ç®—å…¬å¼" width="500"/>
- **æ­¥éª¤**:
```
1.åˆå§‹åŒ–å‚æ•°
2.æ„é€ ä½ç½®ç¼–ç çŸ©é˜µ
  - åˆ›å»ºå…¨é›¶å¼ é‡pe [max-len, d_model] -> [60, 512]
  - æ„é€ ä½ç½®ç´¢å¼•ç´¢å¼•positionå†å‡ç»´ [max_len, 1] ->[60, 1] æ¯è¡Œæ˜¯ä¸€ä¸ªä½ç½® posï¼Œä» 0 åˆ° 59ã€‚
  - è®¡ç®—é¢‘ç‡ç¼©æ”¾å‘é‡(å°±æ˜¯å…¬å¼ä¸­çš„è¢«é™¤æ•°è®¡ç®—)  ->[256]
3.å¡«å……å¶æ•°&å¥‡æ•°ç»´åº¦
  - è®¡ç®—è§’åº¦çŸ©é˜µpos_vec=position * div_term     [max_len, d_model // 2] ->[60, 256]
  - æ‹†åˆ†pe sin/cos
4.æ³¨å†Œä¸ºç¼“å†²åŒº(register_buffer)
  - peå‡ç»´,ä¾¿äºåç»­å¹¿æ’­
  - ä½¿ç”¨register_bufferå°†å…¶åŠ å…¥æ¨¡å‹ï¼Œä½†ä¸ä½œä¸ºå¯è®­ç»ƒå‚æ•°
5.å‰å‘èåˆè¯­ä¹‰å‘é‡å’Œä½ç½®ç¼–ç 
  - å°†è¾“å…¥ xï¼ˆå½¢çŠ¶ [batch, seq_len, d_model]ï¼‰ä¸å¯¹åº”çš„å‰ seq_len ä½ä½ç½®ç¼–ç ç›¸åŠ ï¼Œå†åšéšæœºå¤±æ´»
    åªå–å‰ ğ¿ä¸ªä½ç½®çš„ç¼–ç ï¼Œé¿å…å°†è¶…è¿‡å®é™…å¥é•¿çš„â€œå¤šä½™â€ä½ç½®ç¼–ç åŠ è¿›æ¥ã€‚
```

---

# äºŒ.Encoder

## x.mask(æ©ç å¼ é‡)
- **å‡½æ•°å®šä¹‰**: `subsequent_mask(size)`
```
return 1 - torch.triu(torch.ones(1, size, size, dtype=torch.int), diagonal=1)
```

## 3. Attention(æ³¨æ„åŠ›è®¡ç®—)

- **å‡½æ•°å®šä¹‰**ï¼š`attention(query, key, value, mask, dropout_p)`
- **å…¬å¼**: <img src="https://github.com/user-attachments/assets/e43b185b-bcfb-4635-a279-ae358e2647a5" alt="æ³¨æ„åŠ›æœºåˆ¶å…¬å¼" width="320"/>
- **æ­¥éª¤**:
```
1.è®¡ç®—ç»´åº¦æ ‡é‡ dâ‚–
  - å–æœ€åä¸€ç»´é•¿åº¦ï¼Œåé¢ç”¨æ¥åšç¼©æ”¾å› å­ã€‚
2.ç‚¹ç§¯å¹¶ç¼©æ”¾
  - å°† Query å’Œ Key åšçŸ©é˜µä¹˜æ³•ï¼Œå¾—åˆ°ç›¸ä¼¼åº¦åˆ†æ•°çŸ©é˜µã€‚é™¤ä»¥ğ‘‘ğ‘˜é˜²æ­¢æ•°å€¼è¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±ã€‚
3.åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼‰
  - if  ?  is not None:
4.Softmax å½’ä¸€åŒ–
  - softmaxçš„ä»£ç è®°å¾—dim=-1,å¾—åˆ°æ³¨æ„åŠ›æƒé‡
5.dropout(å¯é€‰)
6.åŠ æƒæ±‚å’Œå¹¶è¿”å›
  - å¾—åˆ°ä¸­é—´ä¸Šä¸‹æ–‡å‘é‡C
```

---
## x.å·¥å…·å‡½æ•°clones
```
module, N
return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])
```
---

## 4. MultiHeadAttention(å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å±‚)

- **ç±»å**ï¼š`MultiHeadAttention`
- **æ„é€ å‡½æ•°**ï¼š`__init__(head, d_model, dropout_p=...)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, query, key, value, mask=None)`
- **å…¬å¼**:<img src="https://github.com/user-attachments/assets/ea220db3-595a-4ff0-8181-64bc12180c34" alt="å¤šå¤´æ³¨æ„åŠ›å…¬å¼" width="400"/>
- **æ­¥éª¤**:
```
init
1. æ–­è¨€æ•´é™¤åˆ¤æ–­ assert
2. å¤šå¤´æ‹†åˆ†ç»´åº¦
3. çº¿æ€§å±‚(q, k, v, èåˆå¤šå¤´åçš„è¾“å‡º)
  - clones()
4. Dropout

forward
1. æ©ç å‡ç»´
2. è·å– batch_size
3. çº¿æ€§å˜æ¢ + åˆ‡åˆ†å¤šå¤´
    #    [batch, seq_len, embed_dim] 
    # â†’ çº¿æ€§ â†’ [batch, seq_len, embed_dim]
    # â†’ view â†’ [batch, seq_len, head, dâ‚–]
    # â†’ transpose â†’ [batch, head, seq_len, dâ‚–]
4. è°ƒç”¨å•å¤´æ³¨æ„åŠ›å‡½æ•°è®¡ç®—
    - è¿”å› shape: [batch, head, seq_len, dâ‚–]
5. æ‹¼å›å¤šå¤´è¾“å‡º
    -    [batch, head, seq_len, dâ‚–] 
    - â†’ transpose â†’ [batch, seq_len, head, dâ‚–]
    - â†’ contiguous.view â†’ [batch, seq_len, embed_dim]
6. returnæœ€åä¸€å±‚çº¿æ€§æ˜ å°„
```

---

## 5. FeedForward(å‰é¦ˆå…¨è¿æ¥å±‚)

- **ç±»å**ï¼š`FeedForward`
- **æ„é€ å‡½æ•°**ï¼š`__init__(d_model, d_ff, dropout_p=...)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x)`
- **å…¬å¼**:<img src="https://github.com/user-attachments/assets/a9aa7e20-4063-4268-a502-ca331506bd8d" alt="å‰é¦ˆå…¨è¿æ¥å±‚å…¬å¼" width="320"/>
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–è¶…å‚æ•°
2. å®šä¹‰ä¸¤å±‚çº¿æ€§å˜æ¢
3. å‰å‘è®¡ç®—æµç¨‹ linear1 -> Rule -> dropout -> linear2
```

---

## 6. LayerNorm(è§„èŒƒåŒ–å±‚)

- **ç±»å**ï¼š`LayerNorm`
- **æ„é€ å‡½æ•°**ï¼š`__init__(features, eps=1e-6)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x)`
- **å…¬å¼**: <img src="https://github.com/user-attachments/assets/b80ff74f-b5d6-4fdd-8d74-138f103bac04" alt="å±‚å½’ä¸€åŒ–å…¬å¼" width="280"/>
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–è¶…å‚æ•°
  - åˆå§‹åŒ–ä¸º å…¨ 1 çš„ç¼©æ”¾å‘é‡ Î³ å’Œ å…¨ 0 çš„åç§»å‘é‡ Î²ã€‚ epsï¼ˆÎµï¼‰ nn.Parameter()
2. è®¡ç®—å‡å€¼ä¸æ ‡å‡†å·®
  - torch.mean/std   keepdim=Ture
3. æ ‡å‡†åŒ–ï¼šå‡å‡å€¼ã€é™¤æ ‡å‡†å·®ï¼ˆåŠ  eps é˜²æ­¢é™¤ 0ï¼‰
  - å†ç”¨ Î³ï¼ˆself.aï¼‰ç¼©æ”¾ã€åŠ  Î²ï¼ˆself.bï¼‰å¹³ç§»
```

---

## 7. SubLayerConnection(å­å±‚è¿æ¥ç»“æ„)

- **ç±»å**ï¼š`SubLayerConnection`
- **æ„é€ å‡½æ•°**ï¼š`__init__(size, dropout_p=...)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x, sublayer)`
- **å…¬å¼**: <img src="https://github.com/user-attachments/assets/cf3910bb-59d1-4c6d-b61e-b257fc04dbb3" alt="æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–" width="400"/>
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–è¶…å‚æ•° & LayerNormæ¨¡å—
2. å‰å‘è®¡ç®—æµç¨‹
    # 1ï¼‰å½’ä¸€åŒ–ï¼šå…ˆå¯¹è¾“å…¥ x åš LayerNorm
    # 2ï¼‰å­å±‚è®¡ç®—ï¼šå°†å½’ä¸€åŒ–ç»“æœè¾“å…¥åˆ°å­å±‚ï¼ˆå¦‚ Attention æˆ– FeedForwardï¼‰
    # 3ï¼‰Dropoutï¼šå¯¹å­å±‚è¾“å‡ºåšéšæœºå¤±æ´»
    # 4ï¼‰æ®‹å·®è¿æ¥ï¼šå°†åŸå§‹ x ä¸å¤„ç†åç»“æœç›¸åŠ 
```

---

## 8. EncoderLayer(ç¼–ç å™¨å±‚)

- **ç±»å**ï¼š`EncoderLayer`
- **æ„é€ å‡½æ•°**ï¼š`__init__(size, self_attention, feed_forward, dropout_p)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x, mask)`
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–å­æ¨¡å—
  -  # å…‹éš†ä¸¤ä¸ª SubLayerConnectionï¼ˆæ®‹å·®+LayerNormï¼‰å®ä¾‹
2. å‰å‘è®¡ç®—æµç¨‹ sub_layer[N]
  - # 1ï¼‰ç¬¬ä¸€å­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›
  - # 2ï¼‰ç¬¬äºŒå­å±‚ï¼šå‰é¦ˆç½‘ç»œ
```

---

## 9. Encoder(ç¼–ç å™¨)

- **ç±»å**ï¼š`Encoder`
- **æ„é€ å‡½æ•°**ï¼š`__init__(layer, N)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x, mask)`
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–æ¨¡å— clones N & è°ƒç”¨LayerNorm
  - layerï¼šå·²æ„é€ çš„ EncoderLayer å®ä¾‹ï¼ˆåŒ…å«è‡ªæ³¨æ„åŠ›ï¼‹å‰é¦ˆï¼‹æ®‹å·®ï¼‹è§„èŒƒåŒ–ï¼‰ã€‚
  - ç”¨ clones å¤åˆ¶ layer N ä»½ï¼Œç”Ÿæˆ ModuleListã€‚
  - æ„é€ æœ€ç»ˆç”¨çš„å±‚å½’ä¸€åŒ– LayerNormï¼Œç»´åº¦å’Œå•å±‚ä¿æŒä¸€è‡´ã€‚
2. å‰å‘è®¡ç®—æµç¨‹
  - å¾ªç¯å †å ï¼šä¾æ¬¡å°†è¾“å…¥ x å’ŒåŒä¸€ mask å–‚å…¥æ¯ä¸€ä¸ªå­ EncoderLayerï¼Œè¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚è¾“å…¥ã€‚
  - å½’ä¸€åŒ–ï¼šæ‰€æœ‰å±‚å¤„ç†å®Œæ¯•åï¼Œå¯¹æœ€ç»ˆè¾“å‡ºåšä¸€æ¬¡ LayerNormï¼Œå¢å¼ºä¿¡æ¯ç¨³å®šæ€§ã€‚
```

---

# ä¸‰.Decoder

---

## 10. DecoderLayer(è§£ç å™¨å±‚)

- **ç±»å**ï¼š`DecoderLayer`
- **æ„é€ å‡½æ•°**ï¼š`__init__(size, self_attention, src_attention, feed_forward, dropout_p)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, y, encoder_output, source_mask, target_mask)`
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–å­æ¨¡å—
  - clones SubLayerConnection
  - ç›®æ ‡åºåˆ—è‡ªæ³¨æ„åŠ›å±‚
  - ç¼–ç å™¨-è§£ç å™¨äº¤äº’æ³¨æ„åŠ›å±‚
2. å‰å‘è®¡ç®—æµç¨‹ N=3ä¸ªå­å±‚
  - sub_layers[?]
```

---

## 11. Decoder(è§£ç å™¨)

- **ç±»å**ï¼š`Decoder`
- **æ„é€ å‡½æ•°**ï¼š`__init__(layer, N)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, y, memory, source_mask, target_mask)`
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–è§£ç å™¨å±‚åˆ—è¡¨ clones layer N=6
2. å®šä¹‰æœ€ç»ˆè§„èŒƒåŒ– LayerNorm
3. å‰å‘è®¡ç®—æµç¨‹
  -  1ï¼‰å¾ªç¯å †å æ¯ä¸€å±‚è§£ç å™¨
  -  2ï¼‰æœ€ç»ˆ LayerNorm å½’ä¸€åŒ–
```

---

## 12. Generator(è¾“å‡º)

- **ç±»å**ï¼š`Generator`
- **æ„é€ å‡½æ•°**ï¼š`__init__(d_model, vocab_size)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, x)`
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–è¶…å‚æ•°
2. å®šä¹‰çº¿æ€§å±‚
3. å‰å‘è®¡ç®—
  - è®°å¾—dim=-1,log_softmax
```
---

# å››. æ•´åˆTransformerå…¨æµç¨‹

- **ç±»å**ï¼š`EncoderToDecoder`
- **æ„é€ å‡½æ•°**ï¼š`___init__(self, encoder, decoder, source_embed, target_embed, generator)`
- **å‰å‘ä¼ æ’­å‡½æ•°**: `forward(self, source_x, target_y, source_mask1, source_mask2, target_mask)`
- **æ­¥éª¤**:
```
1. åˆå§‹åŒ–å­æ¨¡å—
2. å‰å‘è®¡ç®—æµç¨‹
        #  source_x:ä»£è¡¨ç¼–ç å™¨çš„è¾“å…¥ï¼š[batch_size, seq_len]-->[2, 4]
        #  target_y:ä»£è¡¨è§£ç å™¨çš„è¾“å…¥ï¼š[batch_size, seq_len]-->[2, 6]
        #  source_mask1:ä»£è¡¨ç¼–ç å™¨éƒ¨åˆ†çš„padding maskï¼š[head, source_seq_len, source_seq_len]-->[8, 4, 4]
        #  source_mask2:ä»£è¡¨è§£ç å™¨ï¼ˆç¬¬äºŒå­å±‚ï¼‰éƒ¨åˆ†çš„padding maskï¼š[head, target_seq_len, source_seq_len]-->[8, 6, 4]
        #  target_mask:ä»£è¡¨è§£ç å™¨éƒ¨åˆ†çš„sentence maskï¼š[head, target_seq_len, target_seq_len]-->[8, 6, 6]
```

---
---
